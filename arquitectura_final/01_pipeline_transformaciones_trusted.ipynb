{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5b8740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############---------------------Pipeline de Preprocesamiento-------------------####################\n",
    "#1.Ingesta de Datos \n",
    "#2.Selección de Variables\n",
    "#3️.Transformaciones y Tratamiento de Datos\n",
    "#4️.Enriquecimiento de Información\n",
    "#5️.Exportación de Datos Listos para Modelado\n",
    "\n",
    "import pandas as pd\n",
    "import holidays\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class ContugasPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline de procesamiento de datos para análisis de anomalías en redes de gas.\n",
    "    Se incluyen etapas de ingesta, transformación, suavizado de variabilidad, manejo de valores cero \n",
    "    y enriquecimiento con información relevante.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Inicializa la clase con la ruta del archivo de datos.\n",
    "\n",
    "        :param file_path: Ruta del archivo de Excel con los datos.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.data = None\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Carga los datos desde múltiples hojas de un archivo de Excel y los concatena en un DataFrame único.\n",
    "\n",
    "        - Cada hoja representa datos de un cliente.\n",
    "        - Se añade una columna 'Cliente' para indicar la fuente de cada registro.\n",
    "        \"\"\"\n",
    "        excel_data = pd.ExcelFile(self.file_path)\n",
    "        dataframes = []\n",
    "\n",
    "        for sheet_name in excel_data.sheet_names:\n",
    "            df = pd.read_excel(self.file_path, sheet_name=sheet_name)\n",
    "            df['Cliente'] = sheet_name\n",
    "            dataframes.append(df)\n",
    "\n",
    "        self.data = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Selecciona variables clave y preprocesa datos.\n",
    "\n",
    "        - Filtrado de columnas clave ('Fecha', 'Presion', 'Temperatura', 'Volumen', 'Cliente').\n",
    "        - Conversión de Fecha a formato datetime.\n",
    "        - Extracción de Mes y Año para análisis temporal.\n",
    "        - Eliminación de valores faltantes.\n",
    "        \"\"\"\n",
    "        selected_columns = ['Fecha', 'Presion', 'Temperatura', 'Volumen', 'Cliente']\n",
    "        self.data = self.data[selected_columns]\n",
    "        \n",
    "        self.data['Fecha'] = pd.to_datetime(self.data['Fecha'], errors='coerce')\n",
    "        self.data.dropna(inplace=True)\n",
    "        \n",
    "        # Extraer mes y año\n",
    "        self.data['Mes'] = self.data['Fecha'].dt.month\n",
    "        self.data['Año'] = self.data['Fecha'].dt.year\n",
    "\n",
    "    def smooth_variability(self, window=7):\n",
    "        \"\"\"\n",
    "        Suaviza la variabilidad en la presión usando una media móvil.\n",
    "\n",
    "        - Se probaron ventanas de 7, 15 y 30 días, el análisis del notebook de preprocesamiento indicó que 7 días es óptimo.\n",
    "        - Reduce el ruido sin perder tendencias críticas en los datos.\n",
    "        \n",
    "        :param window: Tamaño de la ventana de suavizado (por defecto, 7 días).\n",
    "        \"\"\"\n",
    "        self.data['Presion_Suavizada'] = self.data['Presion'].rolling(window=window, center=True).mean()\n",
    "        \n",
    "        # Elimina columnas innecesarias si existen\n",
    "        self.data.drop(['Presion_Suavizada_15', 'Presion_Suavizada_30'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    def add_holidays(self):\n",
    "        \"\"\"\n",
    "        Incorporación de días feriados en Perú.\n",
    "\n",
    "        - Identifica días festivos en los años presentes en los datos.\n",
    "        - Agrega una columna `Es_Feriado` para evaluar el impacto de días festivos.\n",
    "        \"\"\"\n",
    "        years = self.data['Año'].unique()\n",
    "        peru_holidays = holidays.Peru(years=years)\n",
    "        holiday_dates = {date.strftime(\"%Y-%m-%d\") for date in peru_holidays.keys()}\n",
    "        self.data['Es_Feriado'] = self.data['Fecha'].dt.strftime(\"%Y-%m-%d\").isin(holiday_dates)\n",
    "\n",
    "    def categorize_volume_by_client(self):\n",
    "        \"\"\"\n",
    "        Categorización del volumen de consumo por cliente mediante percentiles.\n",
    "        \"\"\"\n",
    "        clientes = self.data['Cliente'].unique()\n",
    "        \n",
    "        for cliente in clientes:\n",
    "            df_cliente = self.data[self.data['Cliente'] == cliente]\n",
    "            p10 = df_cliente['Volumen'].quantile(0.10)\n",
    "            p90 = df_cliente['Volumen'].quantile(0.90)\n",
    "\n",
    "            self.data.loc[self.data['Cliente'] == cliente, 'Anomalia'] = df_cliente['Volumen'].apply(\n",
    "                lambda x: 'Anómalo' if x < p10 or x > p90 else 'Normal'\n",
    "            )\n",
    "\n",
    "        # Crear columna binaria (1 para anómalo, 0 para normal)\n",
    "        self.data['Anomalia_bin'] = self.data['Anomalia'].map({'Anómalo': 1, 'Normal': 0})\n",
    "\n",
    "    def filter_final_columns(self):\n",
    "        \"\"\"\n",
    "        Filtra las columnas finales requeridas en la salida.\n",
    "        \"\"\"\n",
    "        required_columns = ['Presion', 'Temperatura', 'Volumen', 'Cliente', 'Mes', 'Año', 'Es_Feriado', 'Anomalia', 'Anomalia_bin']\n",
    "        self.data = self.data[required_columns]\n",
    "\n",
    "    def save_clean_data(self, output_path=\"salida/data_cleaned.csv\"):\n",
    "        \"\"\"\n",
    "        Guarda el dataset procesado en un archivo CSV.\n",
    "\n",
    "        :param output_path: Ruta del archivo de salida (por defecto, 'data_cleaned.csv').\n",
    "        \"\"\"\n",
    "        self.data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eac61bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecutando el Pipeline\n",
    "pipeline = ContugasPipeline(r\"insumo\\Contugas_Datos.xlsx\")\n",
    "pipeline.load_data()\n",
    "pipeline.preprocess_data()\n",
    "pipeline.smooth_variability()\n",
    "pipeline.add_holidays()\n",
    "pipeline.categorize_volume_by_client()\n",
    "pipeline.filter_final_columns()\n",
    "pipeline.save_clean_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyect_F_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
