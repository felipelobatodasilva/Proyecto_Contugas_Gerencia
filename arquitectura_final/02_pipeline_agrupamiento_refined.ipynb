{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "568aaddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############---------------------Pipeline de refinamiento-------------------####################\n",
    "#1.Carga de Datos \n",
    "#2.Clusterización de clientes\n",
    "#3️.Preparacion y creacion de proxys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "class ClusterPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline para la transformación refinada de datos antes del modelado.\n",
    "    Incluye carga de datos, clusterización de clientes y asociación para entrenamiento.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, output_dir, num_clusters=5, random_state=42):\n",
    "        \"\"\"\n",
    "        Inicializa el pipeline con parámetros esenciales.\n",
    "\n",
    "        :param file_path: Ruta del archivo CSV con datos procesados previos.\n",
    "        :param output_dir: Directorio donde se guardarán los modelos de clusterización.\n",
    "        :param num_clusters: Número de clusters a generar con KMeans (por defecto, 5).\n",
    "        :param random_state: Estado aleatorio para reproducibilidad.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.output_dir = output_dir\n",
    "        self.num_clusters = num_clusters\n",
    "        self.random_state = random_state\n",
    "        self.data = None\n",
    "        self.summary = None\n",
    "        self.kmeans_model = None\n",
    "        self.scaler = None\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Carga el dataset y verifica la presencia de columnas esenciales.\n",
    "        \n",
    "        - Convierte columnas numéricas para evitar errores de procesamiento.\n",
    "        - Elimina valores faltantes en las columnas claves para clustering.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.data = pd.read_csv(self.file_path)\n",
    "            required_columns = ['Presion', 'Temperatura', 'Volumen', 'Cliente']\n",
    "            \n",
    "            if not all(col in self.data.columns for col in required_columns):\n",
    "                raise ValueError(f\"Faltan columnas esenciales: {required_columns}\")\n",
    "            \n",
    "            # Convertir valores a numérico y eliminar filas con NaN\n",
    "            for col in ['Presion', 'Temperatura', 'Volumen']:\n",
    "                self.data[col] = pd.to_numeric(self.data[col], errors='coerce')\n",
    "                \n",
    "            self.data.dropna(subset=['Presion', 'Temperatura', 'Volumen'], inplace=True)\n",
    "            \n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print(f\" ERROR: Archivo {self.file_path} no encontrado.\")\n",
    "            exit()\n",
    "        except Exception as e:\n",
    "            print(f\" ERROR inesperado: {e}\")\n",
    "            exit()\n",
    "\n",
    "    def create_client_clusters(self):\n",
    "        \"\"\"\n",
    "        Crea clusters de clientes usando KMeans con datos agregados.\n",
    "\n",
    "        - Escala los datos antes de aplicar KMeans.\n",
    "        - Maneja valores faltantes en el resumen mediante imputación.\n",
    "        - Guarda el modelo de clustering y el escalador usado.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.summary = self.data.groupby(\"Cliente\")[['Presion', 'Temperatura', 'Volumen']].mean()\n",
    "            self.scaler = StandardScaler()\n",
    "            summary_scaled = self.scaler.fit_transform(self.summary)\n",
    "\n",
    "            # Imputar valores faltantes si los hay\n",
    "            if np.isnan(summary_scaled).any():\n",
    "                from sklearn.impute import SimpleImputer\n",
    "                imputer = SimpleImputer(strategy='mean')\n",
    "                summary_scaled = imputer.fit_transform(summary_scaled)\n",
    "\n",
    "            self.kmeans_model = KMeans(n_clusters=self.num_clusters, random_state=self.random_state, n_init=10)\n",
    "            labels = self.kmeans_model.fit_predict(summary_scaled)\n",
    "            self.summary['cluster'] = labels\n",
    "            \n",
    "            # Evaluar calidad del clustering\n",
    "            silhouette_avg = silhouette_score(summary_scaled, labels)\n",
    "            print(f\" Coeficiente de Silhouette: {silhouette_avg:.3f}\")\n",
    "\n",
    "            # Guardar modelos\n",
    "            os.makedirs(self.output_dir, exist_ok=True)\n",
    "            joblib.dump(self.kmeans_model, os.path.join(self.output_dir, 'kmeans_model.joblib'))\n",
    "            joblib.dump(self.scaler, os.path.join(self.output_dir, 'scaler.joblib'))\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" ERROR en la clusterización: {e}\")\n",
    "            exit()\n",
    "\n",
    "    def prepare_training_data(self):\n",
    "        \"\"\"\n",
    "        Asocia los clusters de clientes a los registros de consumo y detecta anomalías proxy.\n",
    "\n",
    "        - Realiza la asociación mediante `merge` con la información de clustering.\n",
    "        - Calcula anomalías en cada cluster basado en percentiles (P10/P90).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            summary_reset = self.summary.reset_index()[['Cliente', 'cluster']]\n",
    "            self.data = self.data.merge(summary_reset, on='Cliente', how='left')\n",
    "\n",
    "            # Eliminar registros sin cluster asignado\n",
    "            registros_antes = len(self.data)\n",
    "            self.data.dropna(subset=['cluster'], inplace=True)\n",
    "            registros_despues = len(self.data)\n",
    "            if registros_antes > registros_despues:\n",
    "                print(f\" Se eliminaron {registros_antes - registros_despues} registros sin cluster.\")\n",
    "\n",
    "            # Convertir cluster a entero\n",
    "            self.data['cluster'] = self.data['cluster'].astype(int)\n",
    "\n",
    "            print(\" Calculando 'Anomalia_Proxy_Cluster' dentro de cada cluster...\")\n",
    "            self.data['Anomalia_Proxy_Cluster'] = 0  # Inicializar columna\n",
    "\n",
    "            # Iterar sobre cada cluster y calcular anomalía proxy\n",
    "            for cluster_id in range(self.num_clusters):\n",
    "                cluster_mask = self.data['cluster'] == cluster_id\n",
    "                cluster_data = self.data.loc[cluster_mask, ['Presion', 'Temperatura', 'Volumen']]\n",
    "                \n",
    "                if cluster_data.empty:\n",
    "                    continue  # Si el cluster no tiene datos, omitir\n",
    "\n",
    "                # Calcular percentiles\n",
    "                anomalies = pd.DataFrame(index=cluster_data.index)\n",
    "                for feature in ['Presion', 'Temperatura', 'Volumen']:\n",
    "                    p10, p90 = cluster_data[feature].quantile([0.10, 0.90])\n",
    "                    anomalies[feature] = cluster_data[feature].apply(lambda x: 1 if x < p10 or x > p90 else 0)\n",
    "\n",
    "                # Si cualquiera de las tres variables es anómala, marcar como anomalía\n",
    "                self.data.loc[cluster_mask, 'Anomalia_Proxy_Cluster'] = anomalies.max(axis=1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" ERROR en la preparación de entrenamiento: {e}\")\n",
    "            exit()\n",
    "\n",
    "    def save_training_data(self, output_path=\"salida/training_data.csv\"):\n",
    "        \"\"\"\n",
    "        Guarda los datos preparados en un archivo CSV listo para modelado.\n",
    "\n",
    "        :param output_path: Ruta del archivo de salida (por defecto, 'training_data.csv').\n",
    "        \"\"\"\n",
    "        self.data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2002ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Coeficiente de Silhouette: 0.690\n",
      " Calculando 'Anomalia_Proxy_Cluster' dentro de cada cluster...\n"
     ]
    }
   ],
   "source": [
    "#Ejecutando el Pipeline\n",
    "pipeline = ClusterPipeline(\"salida/data_cleaned.csv\", \"modelos_kmeans5_dinamico_otimizado_viz_otim_sem_hist\")\n",
    "pipeline.load_data()\n",
    "pipeline.create_client_clusters()\n",
    "pipeline.prepare_training_data()\n",
    "pipeline.save_training_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyect_F_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
