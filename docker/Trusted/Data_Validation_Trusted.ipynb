{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5301c2b6-7105-430a-a309-d008c8e58751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.24.4)\n",
      "Collecting holidays\n",
      "  Downloading holidays-0.72-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.3.1)\n",
      "Requirement already satisfied: openpyxl in /opt/conda/lib/python3.11/site-packages (3.1.2)\n",
      "Requirement already satisfied: xlrd in /opt/conda/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: et-xmlfile in /opt/conda/lib/python3.11/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading holidays-0.72-py3-none-any.whl (932 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m932.3/932.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: holidays\n",
      "Successfully installed holidays-0.72\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy holidays scikit-learn openpyxl xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71be9a5a-bcc0-4239-bf9b-a3cffe89d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.0)\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py4j\n",
      "Successfully installed py4j-0.10.9.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7ed8a-1ba0-496a-8b4a-d6e51aac620d",
   "metadata": {},
   "source": [
    "# 1) Pipeline pandas: ingesta, limpieza y exportación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a58f10c6-34e9-499e-b2ab-1392982285e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Ingestão: 847960 linhas carregadas\n",
      "[2] Pré-processamento: colunas atuais ['Fecha', 'Presion', 'Temperatura', 'Volumen', 'Cliente', 'Mes', 'Ano']\n",
      "[3] Suavização: média móvel (7 dias) aplicada\n",
      "[4] Feriados: coluna Es_Feriado criada\n",
      "[5] Anomalias: colunas Anomalia e Anomalia_bin criadas\n",
      "[6] Colunas finais: ['Presion', 'Temperatura', 'Volumen', 'Cliente', 'Mes', 'Ano', 'Es_Feriado', 'Anomalia', 'Anomalia_bin']\n",
      "[7] Salvamento: dados salvos em 'data_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Imports ------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ------------------ Parâmetros ------------------\n",
    "file_path = \"Contugas_Datos.xlsx\"\n",
    "output_path = \"data_cleaned.csv\"\n",
    "\n",
    "# ------------------ 1. Ingestão de dados ------------------\n",
    "excel = pd.ExcelFile(file_path)\n",
    "lista_dfs = []\n",
    "\n",
    "for sheet in excel.sheet_names:\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet)\n",
    "    df['Cliente'] = sheet\n",
    "    lista_dfs.append(df)\n",
    "\n",
    "data = pd.concat(lista_dfs, ignore_index=True)\n",
    "print(f\"[1] Ingestão: {len(data)} linhas carregadas\")\n",
    "\n",
    "# ------------------ 2. Pré-processamento ------------------\n",
    "# Seleção de colunas\n",
    "data = data[['Fecha', 'Presion', 'Temperatura', 'Volumen', 'Cliente']].copy()\n",
    "\n",
    "# Conversão de data e remoção de nulos\n",
    "data['Fecha'] = pd.to_datetime(data['Fecha'], errors='coerce')\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Extração de mês e ano\n",
    "data['Mes'] = data['Fecha'].dt.month\n",
    "data['Ano'] = data['Fecha'].dt.year\n",
    "print(f\"[2] Pré-processamento: colunas atuais {list(data.columns)}\")\n",
    "\n",
    "# ------------------ 3. Suavização da variabilidade ------------------\n",
    "window = 7  # ajustável\n",
    "data['Presion_Suavizada'] = data['Presion'].rolling(window=window, center=True).mean()\n",
    "# remover colunas antigas se existirem\n",
    "data.drop(['Presion_Suavizada_15', 'Presion_Suavizada_30'], axis=1, errors='ignore', inplace=True)\n",
    "print(f\"[3] Suavização: média móvel ({window} dias) aplicada\")\n",
    "\n",
    "# ------------------ 4. Adição de feriados ------------------\n",
    "anos = data['Ano'].unique().tolist()\n",
    "feriados_pe = holidays.Peru(years=anos)\n",
    "datas_feriados = {d.strftime(\"%Y-%m-%d\") for d in feriados_pe.keys()}\n",
    "data['Es_Feriado'] = data['Fecha'].dt.strftime(\"%Y-%m-%d\").isin(datas_feriados)\n",
    "print(\"[4] Feriados: coluna Es_Feriado criada\")\n",
    "\n",
    "# ------------------ 5. Categorização de volume por cliente ------------------\n",
    "for cliente in data['Cliente'].unique():\n",
    "    sub = data[data['Cliente'] == cliente]\n",
    "    p10, p90 = sub['Volumen'].quantile(0.10), sub['Volumen'].quantile(0.90)\n",
    "    mask = data['Cliente'] == cliente\n",
    "    data.loc[mask, 'Anomalia'] = sub['Volumen'].apply(\n",
    "        lambda x: 'Anômalo' if x < p10 or x > p90 else 'Normal'\n",
    "    )\n",
    "data['Anomalia_bin'] = data['Anomalia'].map({'Anômalo': 1, 'Normal': 0})\n",
    "print(\"[5] Anomalias: colunas Anomalia e Anomalia_bin criadas\")\n",
    "\n",
    "# ------------------ 6. Filtragem de colunas finais ------------------\n",
    "col_final = [\n",
    "    'Presion', 'Temperatura', 'Volumen', 'Cliente',\n",
    "    'Mes', 'Ano', 'Es_Feriado', 'Anomalia', 'Anomalia_bin'\n",
    "]\n",
    "data = data[col_final].copy()\n",
    "print(f\"[6] Colunas finais: {col_final}\")\n",
    "\n",
    "# ------------------ 7. Salvamento ------------------\n",
    "data.to_csv(output_path, index=False)\n",
    "print(f\"[7] Salvamento: dados salvos em '{output_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c5c82-787b-4537-ad61-f93a4a49fa04",
   "metadata": {},
   "source": [
    "# 2) Validación de datos Generados con Pandas por medio de Spark SQL (Pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be931349-a385-4645-98ad-76a003b5f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. Criar SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ContugasTest\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Ler o CSV gerado\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"data_cleaned.csv\")  # ajuste o caminho se estiver em S3: \"s3://meu-bucket/data_cleaned.csv\"\n",
    "\n",
    "# 3. Criar view temporária (nome arbitrário)\n",
    "df.createOrReplaceTempView(\"contugas_pandas_procedado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c813c5a1-d1fc-499d-a958-20ff29cb6775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+--------+---+----+----------+--------+------------+\n",
      "|Presion          |Temperatura      |Volumen          |Cliente |Mes|Ano |Es_Feriado|Anomalia|Anomalia_bin|\n",
      "+-----------------+-----------------+-----------------+--------+---+----+----------+--------+------------+\n",
      "|17.7325634924889 |28.2093536541928 |20.96975076975659|CLIENTE1|1  |2019|false     |Normal  |0           |\n",
      "|17.74777603806793|28.51861421312152|17.84573913758869|CLIENTE1|1  |2019|false     |Normal  |0           |\n",
      "|17.75891638774564|28.23019056507057|20.97591383522386|CLIENTE1|1  |2019|false     |Normal  |0           |\n",
      "|17.72794022684193|27.8115085858999 |20.59229909002912|CLIENTE1|1  |2019|false     |Normal  |0           |\n",
      "|17.74648374408114|27.79529343321371|21.69062581523098|CLIENTE1|1  |2019|false     |Normal  |0           |\n",
      "|17.73554042397516|27.66645651943054|19.09171697331939|CLIENTE1|1  |2019|false     |Normal  |0           |\n",
      "|17.73179953287394|27.53234547004581|18.94188520855138|CLIENTE1|1  |2019|false     |Normal  |0           |\n",
      "|17.71530664648446|27.27956374846896|14.99414619209886|CLIENTE1|1  |2019|false     |Normal  |0           |\n",
      "|17.70288300304363|27.08497753285612|19.29994101318612|CLIENTE1|1  |2019|false     |Normal  |0           |\n",
      "|17.71153158996626|28.82525706474128|19.40866744770924|CLIENTE1|1  |2019|false     |Normal  |0           |\n",
      "+-----------------+-----------------+-----------------+--------+---+----+----------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 1: Mostrar las primeras 10 filas de la tabla\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM contugas_pandas_procedado\n",
    "    LIMIT 10\n",
    "\"\"\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69aad423-3efc-4ef1-a55d-001b40fd9324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|Anomalia|total |\n",
      "+--------+------+\n",
      "|Anômalo |131750|\n",
      "|Normal  |716210|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 2: Contar el número total de registros por tipo de anomalía\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Anomalia, COUNT(*) AS total\n",
    "    FROM contugas_pandas_procedado\n",
    "    GROUP BY Anomalia\n",
    "    ORDER BY Anomalia\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8093926e-dddd-42c6-9297-affcd030cb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+\n",
      "|Cliente  |Anomalia|total|\n",
      "+---------+--------+-----+\n",
      "|CLIENTE1 |Anômalo |8684 |\n",
      "|CLIENTE1 |Normal  |34728|\n",
      "|CLIENTE10|Anômalo |4106 |\n",
      "|CLIENTE10|Normal  |36953|\n",
      "|CLIENTE11|Anômalo |8450 |\n",
      "|CLIENTE11|Normal  |33798|\n",
      "|CLIENTE12|Anômalo |4178 |\n",
      "|CLIENTE12|Normal  |37598|\n",
      "|CLIENTE13|Anômalo |4106 |\n",
      "|CLIENTE13|Normal  |36953|\n",
      "|CLIENTE14|Anômalo |8684 |\n",
      "|CLIENTE14|Normal  |34731|\n",
      "|CLIENTE15|Anômalo |4243 |\n",
      "|CLIENTE15|Normal  |38185|\n",
      "|CLIENTE16|Anômalo |8630 |\n",
      "|CLIENTE16|Normal  |34517|\n",
      "|CLIENTE17|Anômalo |8684 |\n",
      "|CLIENTE17|Normal  |34728|\n",
      "|CLIENTE18|Anômalo |8278 |\n",
      "|CLIENTE18|Normal  |33104|\n",
      "+---------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 3: Contar registros agrupados por cliente y tipo de anomalía\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Cliente, Anomalia, COUNT(*) AS total\n",
    "    FROM contugas_pandas_procedado\n",
    "    GROUP BY Cliente, Anomalia\n",
    "    ORDER BY Cliente, Anomalia\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b49ac91a-39d3-40b9-8589-5f28296b6e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+------------------+\n",
      "|total_linhas|clientes_distintos|periodos_distintos|\n",
      "+------------+------------------+------------------+\n",
      "|      847960|                20|                60|\n",
      "+------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 4: Verificar el total de filas, clientes distintos y periodos distintos\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      COUNT(*) AS total_linhas,\n",
    "      COUNT(DISTINCT Cliente) AS clientes_distintos,\n",
    "      COUNT(DISTINCT CONCAT(Ano,'-',Mes)) AS periodos_distintos\n",
    "    FROM contugas_pandas_procedado\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1adc060-ad17-458d-9044-23e0b30aab73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|Anomalia_bin|total |\n",
      "+------------+------+\n",
      "|0           |716210|\n",
      "|1           |131750|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 5: Distribución de anomalia_bin y conteo total por cada valor\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Anomalia_bin, COUNT(*) AS total\n",
    "    FROM contugas_pandas_procedado\n",
    "    GROUP BY Anomalia_bin\n",
    "    ORDER BY Anomalia_bin\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0f27275-2681-41f8-a4bd-29c1c99dfb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|Anomalia|Anomalia_bin|total |\n",
      "+--------+------------+------+\n",
      "|Anômalo |1           |131750|\n",
      "|Normal  |0           |716210|\n",
      "+--------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 6: Comprobar consistencia entre Anomalia y Anomalia_bin\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Anomalia, Anomalia_bin, COUNT(*) AS total\n",
    "    FROM contugas_pandas_procedado\n",
    "    GROUP BY Anomalia, Anomalia_bin\n",
    "    ORDER BY Anomalia, Anomalia_bin\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "732c6f5b-3690-4cbb-b140-8de2ccaf8017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+--------+--------+--------+-------+-------+-------+\n",
      "|avg_presion|min_presion|max_presion|avg_temp|min_temp|max_temp|avg_vol|min_vol|max_vol|\n",
      "+-----------+-----------+-----------+--------+--------+--------+-------+-------+-------+\n",
      "|16.07      |2.93       |20.31      |25.2    |-5.26   |50.02   |62.33  |0.0    |577.41 |\n",
      "+-----------+-----------+-----------+--------+--------+--------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 7: Obtener estadísticas (promedio, mínimo, máximo) de Presion, Temperatura y Volumen\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      ROUND(AVG(Presion),2)     AS avg_presion,\n",
    "      ROUND(MIN(Presion),2)     AS min_presion,\n",
    "      ROUND(MAX(Presion),2)     AS max_presion,\n",
    "      ROUND(AVG(Temperatura),2) AS avg_temp,\n",
    "      ROUND(MIN(Temperatura),2) AS min_temp,\n",
    "      ROUND(MAX(Temperatura),2) AS max_temp,\n",
    "      ROUND(AVG(Volumen),2)     AS avg_vol,\n",
    "      ROUND(MIN(Volumen),2)     AS min_vol,\n",
    "      ROUND(MAX(Volumen),2)     AS max_vol\n",
    "    FROM contugas_pandas_procedado\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "329bb357-45eb-4031-81ae-22d1518450a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|Cliente  |pct_anomalia|\n",
      "+---------+------------+\n",
      "|CLIENTE1 |20.0        |\n",
      "|CLIENTE10|10.0        |\n",
      "|CLIENTE11|20.0        |\n",
      "|CLIENTE12|10.0        |\n",
      "|CLIENTE13|10.0        |\n",
      "|CLIENTE14|20.0        |\n",
      "|CLIENTE15|10.0        |\n",
      "|CLIENTE16|20.0        |\n",
      "|CLIENTE17|20.0        |\n",
      "|CLIENTE18|20.0        |\n",
      "|CLIENTE19|10.0        |\n",
      "|CLIENTE2 |20.0        |\n",
      "|CLIENTE20|20.0        |\n",
      "|CLIENTE3 |20.0        |\n",
      "|CLIENTE4 |10.0        |\n",
      "|CLIENTE5 |10.0        |\n",
      "|CLIENTE6 |20.0        |\n",
      "|CLIENTE7 |10.0        |\n",
      "|CLIENTE8 |20.0        |\n",
      "|CLIENTE9 |10.0        |\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 8: Calcular el porcentaje de anomalías por cliente\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      Cliente,\n",
    "      ROUND(100 * SUM(Anomalia_bin) / COUNT(*),2) AS pct_anomalia\n",
    "    FROM contugas_pandas_procedado\n",
    "    GROUP BY Cliente\n",
    "    ORDER BY Cliente\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f48a7234-6833-44e6-8fe2-eebd6258b5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+------------+\n",
      "|Ano |Mes|total|pct_anomalia|\n",
      "+----+---+-----+------------+\n",
      "|2019|1  |8622 |19.37       |\n",
      "|2019|2  |13304|16.84       |\n",
      "|2019|3  |14530|17.89       |\n",
      "|2019|4  |12328|12.41       |\n",
      "|2019|5  |14816|11.43       |\n",
      "|2019|6  |14166|12.35       |\n",
      "|2019|7  |14730|13.05       |\n",
      "|2019|8  |14612|11.15       |\n",
      "|2019|9  |12990|12.16       |\n",
      "|2019|10 |13330|12.61       |\n",
      "|2019|11 |14288|12.84       |\n",
      "|2019|12 |14754|13.43       |\n",
      "|2020|1  |14810|19.01       |\n",
      "|2020|2  |13728|16.79       |\n",
      "|2020|3  |14766|17.21       |\n",
      "|2020|4  |14342|17.81       |\n",
      "|2020|5  |14774|16.47       |\n",
      "|2020|6  |14194|14.32       |\n",
      "|2020|7  |14602|11.37       |\n",
      "|2020|8  |14590|11.8        |\n",
      "+----+---+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 9: Evolución mensual de anomalías (total y porcentaje)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      Ano,\n",
    "      Mes,\n",
    "      COUNT(*) AS total,\n",
    "      ROUND(100 * SUM(Anomalia_bin) / COUNT(*),2) AS pct_anomalia\n",
    "    FROM contugas_pandas_procedado\n",
    "    GROUP BY Ano, Mes\n",
    "    ORDER BY Ano, Mes\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28664098-8fe0-4df3-b67e-0c31829b390d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|Es_Feriado|total |pct  |\n",
      "+----------+------+-----+\n",
      "|false     |816302|96.27|\n",
      "|true      |31658 |3.73 |\n",
      "+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 10: Distribución entre días festivos y días laborables con porcentaje sobre el total\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      Es_Feriado,\n",
    "      COUNT(*) AS total,\n",
    "      ROUND(100 * COUNT(*) / (SELECT COUNT(*) FROM contugas_trusted_dato_procesado),2) AS pct\n",
    "    FROM contugas_pandas_procedado\n",
    "    GROUP BY Es_Feriado\n",
    "    ORDER BY Es_Feriado\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b9f14-9f6b-4c8f-885c-2330ec5bf1f5",
   "metadata": {},
   "source": [
    "# 3) Validación de datos Generados con AWS Glue/Pyspark por medio de Spark SQL (Pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4496a0ce-084d-4b75-9647-e9f311d6f3ee",
   "metadata": {},
   "source": [
    "Estos archivos que se leerán a continuación fueron extraídos del bucket de la capa trusted del data lake para validar los datos, es decir, para comprobar si lo que se generó con Glue y PySpark coincide con lo que se había hecho inicialmente con pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cff2e56-febf-428e-9a7b-19da54c30c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined = (\n",
    "    spark.read\n",
    "         .parquet(\"Parquet_Trusted_Files/part-*.snappy.parquet\")\n",
    "         .withColumnRenamed(\"año\", \"ano\")\n",
    ")\n",
    "\n",
    "df_refined.createOrReplaceTempView(\"contugas_trusted_dato_procesado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ed42994-ffe8-4171-9d74-9a3e45fcc7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----------------+-----------------+-----------------+---+----+------------------+----------+------------------+------------------+--------+------------+\n",
      "|cliente  |fecha              |presion          |temperatura      |volumen          |mes|ano |presion_suavizada |es_feriado|p10               |p90               |anomalia|anomalia_bin|\n",
      "+---------+-------------------+-----------------+-----------------+-----------------+---+----+------------------+----------+------------------+------------------+--------+------------+\n",
      "|CLIENTE17|2019-01-19 00:00:00|17.77212441594732|27.85500379522216|23.68536560418588|1  |2019|17.533307762226894|false     |11.290658237352273|26.359754860759395|Normal  |0           |\n",
      "|CLIENTE17|2019-01-19 01:00:00|17.78868605831389|27.98359695831391|22.82341722448241|1  |2019|17.59235216149999 |false     |11.290658237352273|26.359754860759395|Normal  |0           |\n",
      "|CLIENTE17|2019-01-19 02:00:00|17.85616205512526|27.94465807933698|20.15624166244635|1  |2019|17.612018661313876|false     |11.290658237352273|26.359754860759395|Normal  |0           |\n",
      "|CLIENTE17|2019-01-19 03:00:00|17.70745451813866|27.94474028993267|22.68707191743945|1  |2019|17.631426652181688|false     |11.290658237352273|26.359754860759395|Normal  |0           |\n",
      "|CLIENTE17|2019-01-19 04:00:00|17.71015906738997|27.74992135460833|21.97676618646593|1  |2019|17.658388645596403|false     |11.290658237352273|26.359754860759395|Normal  |0           |\n",
      "|CLIENTE17|2019-01-19 05:00:00|17.74958094688507|27.91651266955689|28.89082280501933|1  |2019|17.619157812922037|false     |11.290658237352273|26.359754860759395|Anómalo |1           |\n",
      "|CLIENTE17|2019-01-19 06:00:00|17.74367965258029|27.60810605537412|22.53851989356853|1  |2019|17.586210192871548|false     |11.290658237352273|26.359754860759395|Normal  |0           |\n",
      "|CLIENTE17|2019-01-19 07:00:00|17.59150787170958|27.71616114492751|21.23341862914484|1  |2019|17.484455288420687|false     |11.290658237352273|26.359754860759395|Normal  |0           |\n",
      "|CLIENTE17|2019-01-19 08:00:00|17.70518548549723|27.4794344184817 |20.90613319018286|1  |2019|17.6036141950277  |false     |11.290658237352273|26.359754860759395|Normal  |0           |\n",
      "|CLIENTE17|2019-01-19 09:00:00|17.75498423556387|27.61114229533394|23.08958073670587|1  |2019|17.502587863987877|false     |11.290658237352273|26.359754860759395|Normal  |0           |\n",
      "+---------+-------------------+-----------------+-----------------+-----------------+---+----+------------------+----------+------------------+------------------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 1: Mostrar las primeras 10 filas de la tabla\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM contugas_trusted_dato_procesado\n",
    "    LIMIT 10\n",
    "\"\"\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0db2cf02-8c43-4ee1-aaf1-5406a7c278ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|Anomalia|total |\n",
      "+--------+------+\n",
      "|Anómalo |131750|\n",
      "|Normal  |716210|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 2: Contar el número total de registros por tipo de anomalía\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Anomalia, COUNT(*) AS total\n",
    "    FROM contugas_trusted_dato_procesado\n",
    "    GROUP BY Anomalia\n",
    "    ORDER BY Anomalia\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb61b679-15e0-41d7-a93c-dad80b069974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+\n",
      "|Cliente  |Anomalia|total|\n",
      "+---------+--------+-----+\n",
      "|CLIENTE1 |Anómalo |8684 |\n",
      "|CLIENTE1 |Normal  |34728|\n",
      "|CLIENTE10|Anómalo |4106 |\n",
      "|CLIENTE10|Normal  |36953|\n",
      "|CLIENTE11|Anómalo |8450 |\n",
      "|CLIENTE11|Normal  |33798|\n",
      "|CLIENTE12|Anómalo |4178 |\n",
      "|CLIENTE12|Normal  |37598|\n",
      "|CLIENTE13|Anómalo |4106 |\n",
      "|CLIENTE13|Normal  |36953|\n",
      "|CLIENTE14|Anómalo |8684 |\n",
      "|CLIENTE14|Normal  |34731|\n",
      "|CLIENTE15|Anómalo |4243 |\n",
      "|CLIENTE15|Normal  |38185|\n",
      "|CLIENTE16|Anómalo |8630 |\n",
      "|CLIENTE16|Normal  |34517|\n",
      "|CLIENTE17|Anómalo |8684 |\n",
      "|CLIENTE17|Normal  |34728|\n",
      "|CLIENTE18|Anómalo |8278 |\n",
      "|CLIENTE18|Normal  |33104|\n",
      "+---------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 3: Contar registros agrupados por cliente y tipo de anomalía\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Cliente, Anomalia, COUNT(*) AS total\n",
    "    FROM contugas_trusted_dato_procesado\n",
    "    GROUP BY Cliente, Anomalia\n",
    "    ORDER BY Cliente, Anomalia\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccd718ae-7b10-41bb-a5e9-7e4d38f85b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+------------------+\n",
      "|total_linhas|clientes_distintos|periodos_distintos|\n",
      "+------------+------------------+------------------+\n",
      "|      847960|                20|                60|\n",
      "+------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 4: Verificar el total de filas, clientes distintos y periodos distintos\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      COUNT(*) AS total_linhas,\n",
    "      COUNT(DISTINCT Cliente) AS clientes_distintos,\n",
    "      COUNT(DISTINCT CONCAT(Ano,'-',Mes)) AS periodos_distintos\n",
    "    FROM contugas_trusted_dato_procesado\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26b395be-5b6e-493a-959c-af42415d2e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|Anomalia_bin|total |\n",
      "+------------+------+\n",
      "|0           |716210|\n",
      "|1           |131750|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 5: Distribución de anomalia_bin y conteo total por cada valor\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Anomalia_bin, COUNT(*) AS total\n",
    "    FROM contugas_trusted_dato_procesado\n",
    "    GROUP BY Anomalia_bin\n",
    "    ORDER BY Anomalia_bin\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2797b6be-a35c-4722-8a7c-19ec7a5e5b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|Anomalia|Anomalia_bin|total |\n",
      "+--------+------------+------+\n",
      "|Anómalo |1           |131750|\n",
      "|Normal  |0           |716210|\n",
      "+--------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 6: Comprobar consistencia entre Anomalia y Anomalia_bin\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Anomalia, Anomalia_bin, COUNT(*) AS total\n",
    "    FROM contugas_trusted_dato_procesado\n",
    "    GROUP BY Anomalia, Anomalia_bin\n",
    "    ORDER BY Anomalia, Anomalia_bin\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0daac17-fb12-494f-ba96-d2ce7eb66801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+--------+--------+--------+-------+-------+-------+\n",
      "|avg_presion|min_presion|max_presion|avg_temp|min_temp|max_temp|avg_vol|min_vol|max_vol|\n",
      "+-----------+-----------+-----------+--------+--------+--------+-------+-------+-------+\n",
      "|16.07      |2.93       |20.31      |25.2    |-5.26   |50.02   |62.33  |0.0    |577.41 |\n",
      "+-----------+-----------+-----------+--------+--------+--------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 7: Obtener estadísticas (promedio, mínimo, máximo) de Presion, Temperatura y Volumen\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      ROUND(AVG(Presion),2)     AS avg_presion,\n",
    "      ROUND(MIN(Presion),2)     AS min_presion,\n",
    "      ROUND(MAX(Presion),2)     AS max_presion,\n",
    "      ROUND(AVG(Temperatura),2) AS avg_temp,\n",
    "      ROUND(MIN(Temperatura),2) AS min_temp,\n",
    "      ROUND(MAX(Temperatura),2) AS max_temp,\n",
    "      ROUND(AVG(Volumen),2)     AS avg_vol,\n",
    "      ROUND(MIN(Volumen),2)     AS min_vol,\n",
    "      ROUND(MAX(Volumen),2)     AS max_vol\n",
    "    FROM contugas_trusted_dato_procesado\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87b59eb3-d16e-4582-9f0d-a872d322186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|Cliente  |pct_anomalia|\n",
      "+---------+------------+\n",
      "|CLIENTE1 |20.0        |\n",
      "|CLIENTE10|10.0        |\n",
      "|CLIENTE11|20.0        |\n",
      "|CLIENTE12|10.0        |\n",
      "|CLIENTE13|10.0        |\n",
      "|CLIENTE14|20.0        |\n",
      "|CLIENTE15|10.0        |\n",
      "|CLIENTE16|20.0        |\n",
      "|CLIENTE17|20.0        |\n",
      "|CLIENTE18|20.0        |\n",
      "|CLIENTE19|10.0        |\n",
      "|CLIENTE2 |20.0        |\n",
      "|CLIENTE20|20.0        |\n",
      "|CLIENTE3 |20.0        |\n",
      "|CLIENTE4 |10.0        |\n",
      "|CLIENTE5 |10.0        |\n",
      "|CLIENTE6 |20.0        |\n",
      "|CLIENTE7 |10.0        |\n",
      "|CLIENTE8 |20.0        |\n",
      "|CLIENTE9 |10.0        |\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 8: Calcular el porcentaje de anomalías por cliente\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      Cliente,\n",
    "      ROUND(100 * SUM(Anomalia_bin) / COUNT(*),2) AS pct_anomalia\n",
    "    FROM contugas_trusted_dato_procesado\n",
    "    GROUP BY Cliente\n",
    "    ORDER BY Cliente\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6141904-0161-440d-878f-4f204698d364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+------------+\n",
      "|Ano |Mes|total|pct_anomalia|\n",
      "+----+---+-----+------------+\n",
      "|2019|1  |8622 |19.37       |\n",
      "|2019|2  |13304|16.84       |\n",
      "|2019|3  |14530|17.89       |\n",
      "|2019|4  |12328|12.41       |\n",
      "|2019|5  |14816|11.43       |\n",
      "|2019|6  |14166|12.35       |\n",
      "|2019|7  |14730|13.05       |\n",
      "|2019|8  |14612|11.15       |\n",
      "|2019|9  |12990|12.16       |\n",
      "|2019|10 |13330|12.61       |\n",
      "|2019|11 |14288|12.84       |\n",
      "|2019|12 |14754|13.43       |\n",
      "|2020|1  |14810|19.01       |\n",
      "|2020|2  |13728|16.79       |\n",
      "|2020|3  |14766|17.21       |\n",
      "|2020|4  |14342|17.81       |\n",
      "|2020|5  |14774|16.47       |\n",
      "|2020|6  |14194|14.32       |\n",
      "|2020|7  |14602|11.37       |\n",
      "|2020|8  |14590|11.8        |\n",
      "+----+---+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 9: Evolución mensual de anomalías (total y porcentaje)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      Ano,\n",
    "      Mes,\n",
    "      COUNT(*) AS total,\n",
    "      ROUND(100 * SUM(Anomalia_bin) / COUNT(*),2) AS pct_anomalia\n",
    "    FROM contugas_trusted_dato_procesado\n",
    "    GROUP BY Ano, Mes\n",
    "    ORDER BY Ano, Mes\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19e1cb54-1872-46a0-92f9-43aef8a0662a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|Es_Feriado|total |pct  |\n",
      "+----------+------+-----+\n",
      "|false     |816302|96.27|\n",
      "|true      |31658 |3.73 |\n",
      "+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta 10: Distribución entre días festivos y días laborables con porcentaje sobre el total\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      Es_Feriado,\n",
    "      COUNT(*) AS total,\n",
    "      ROUND(100 * COUNT(*) / (SELECT COUNT(*) FROM contugas_trusted_dato_procesado),2) AS pct\n",
    "    FROM contugas_trusted_dato_procesado\n",
    "    GROUP BY Es_Feriado\n",
    "    ORDER BY Es_Feriado\n",
    "\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
