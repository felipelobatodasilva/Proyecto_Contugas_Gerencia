{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80f39268-38af-4ea0-88bd-60703c7df3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.24.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.3.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy scikit-learn joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dcf91a-7df0-43e9-8985-3192049a104d",
   "metadata": {},
   "source": [
    "# 1) Pipeline pandas: clusterización y preparación de datos para entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0306fdbb-0978-465d-be32-f5b60ef1d142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Ingestão: 847960 → 847960 registros válidos\n",
      "[2] Silhouette score: 0.690\n",
      "[2] Modelos salvos em 'refined_models'\n",
      "[3] Eliminados 0 registros sem cluster\n",
      "[3] Coluna 'Anomalia_Proxy_Cluster' calculada\n",
      "[4] Dados de treinamento salvos em 'training_data.csv'\n",
      "[1] Ingesta: 847960 → 847960 registros válidos\n",
      "[2] Puntuación Silhouette: 0.690\n",
      "[2] Modelos guardados en 'refined_models'\n",
      "[3] Eliminados 0 registros sin cluster\n",
      "[3] Columna 'Anomalia_Proxy_Cluster' calculada\n",
      "[4] Datos de entrenamiento guardados en 'training_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Imports ------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# ------------------ Parâmetros ------------------\n",
    "file_path    = \"data_cleaned.csv\"    # CSV de entrada (output do pipeline anterior)\n",
    "output_dir   = \"refined_models\"      # onde salvar kmeans_model.joblib e scaler.joblib\n",
    "num_clusters = 5                     # número de clusters\n",
    "random_state = 42                    # semente para reprodutibilidade\n",
    "\n",
    "# ------------------ 1. Ingestão de dados ------------------\n",
    "data = pd.read_csv(file_path)\n",
    "required = ['Presion', 'Temperatura', 'Volumen', 'Cliente']\n",
    "missing  = [c for c in required if c not in data.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Faltam colunas essenciais: {missing}\")\n",
    "\n",
    "# converter colunas numéricas e dropar linhas inválidas\n",
    "for c in ['Presion', 'Temperatura', 'Volumen']:\n",
    "    data[c] = pd.to_numeric(data[c], errors='coerce')\n",
    "before = len(data)\n",
    "data.dropna(subset=['Presion', 'Temperatura', 'Volumen'], inplace=True)\n",
    "after = len(data)\n",
    "print(f\"[1] Ingestão: {before} → {after} registros válidos\")\n",
    "\n",
    "# ------------------ 2. Clusterização de clientes ------------------\n",
    "# 2.1 Resumo médio por cliente\n",
    "summary = data.groupby('Cliente')[['Presion','Temperatura','Volumen']].mean()\n",
    "\n",
    "# 2.2 Escalonamento\n",
    "scaler   = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(summary)\n",
    "\n",
    "# 2.3 Imputação de NaNs, se houver\n",
    "if np.isnan(X_scaled).any():\n",
    "    imputer  = SimpleImputer(strategy='mean')\n",
    "    X_scaled = imputer.fit_transform(X_scaled)\n",
    "\n",
    "# 2.4 KMeans\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=random_state, n_init=10)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "summary['cluster'] = labels\n",
    "\n",
    "# 2.5 Avaliação com Silhouette\n",
    "sil = silhouette_score(X_scaled, labels)\n",
    "print(f\"[2] Silhouette score: {sil:.3f}\")\n",
    "\n",
    "# 2.6 Salvando modelos\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "joblib.dump(kmeans, os.path.join(output_dir, 'kmeans_model.joblib'))\n",
    "joblib.dump(scaler, os.path.join(output_dir, 'scaler.joblib'))\n",
    "print(f\"[2] Modelos salvos em '{output_dir}'\")\n",
    "\n",
    "# ------------------ 3. Preparação dos dados para treinamento ------------------\n",
    "# 3.1 Merge dos clusters no dataset original\n",
    "df_clusters = summary.reset_index()[['Cliente','cluster']]\n",
    "data = data.merge(df_clusters, on='Cliente', how='left')\n",
    "\n",
    "# 3.2 Remover registros sem cluster\n",
    "b = len(data)\n",
    "data.dropna(subset=['cluster'], inplace=True)\n",
    "a = len(data)\n",
    "print(f\"[3] Eliminados {b-a} registros sem cluster\")\n",
    "data['cluster'] = data['cluster'].astype(int)\n",
    "\n",
    "# 3.3 Cálculo de Anomalia_Proxy_Cluster\n",
    "data['Anomalia_Proxy_Cluster'] = 0\n",
    "for cid in range(num_clusters):\n",
    "    mask = data['cluster'] == cid\n",
    "    sub  = data.loc[mask, ['Presion','Temperatura','Volumen']]\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    flags = pd.DataFrame(index=sub.index)\n",
    "    for feat in ['Presion','Temperatura','Volumen']:\n",
    "        p10, p90 = sub[feat].quantile([0.10, 0.90])\n",
    "        flags[feat] = sub[feat].apply(lambda x: 1 if x < p10 or x > p90 else 0)\n",
    "    data.loc[mask, 'Anomalia_Proxy_Cluster'] = flags.max(axis=1)\n",
    "print(\"[3] Coluna 'Anomalia_Proxy_Cluster' calculada\")\n",
    "\n",
    "# ------------------ 3.4 Renomear colunas para o formato final ------------------\n",
    "data.rename(columns={\n",
    "    'Cliente': 'cliente',\n",
    "    'Fecha': 'fecha',                 # caso exista a coluna Fecha\n",
    "    'Presion': 'presion',\n",
    "    'Temperatura': 'temperatura',\n",
    "    'Volumen': 'volumen',\n",
    "    'Mes': 'mes',\n",
    "    'Ano': 'año',                     # se a coluna se chamar Ano\n",
    "    'Presion_Suavizada': 'presion_suavizada',\n",
    "    'Es_Feriado': 'es_feriado',\n",
    "    'Anomalia': 'anomalia',\n",
    "    'Anomalia_bin': 'anomalia_bin'\n",
    "}, inplace=True)\n",
    "# As colunas já criadas abaixo já têm o nome correto:\n",
    "# 'cluster', 'anomalia_presion', 'anomalia_temperatura',\n",
    "# 'anomalia_volumen', 'anomalia_proxy_cluster'\n",
    "\n",
    "# ------------------ 4. Salvamento final ------------------\n",
    "training_path = \"training_data.csv\"\n",
    "data.to_csv(training_path, index=False)\n",
    "print(f\"[4] Dados de treinamento salvos em '{training_path}'\")# ------------------ Importaciones ------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# ------------------ Parámetros ------------------\n",
    "file_path    = \"data_cleaned.csv\"    # CSV de entrada (output del pipeline anterior)\n",
    "output_dir   = \"refined_models\"      # dónde guardar kmeans_model.joblib y scaler.joblib\n",
    "num_clusters = 5                     # número de clusters\n",
    "random_state = 42                    # semilla para reproducibilidad\n",
    "\n",
    "# ------------------ 1. Ingesta de datos ------------------\n",
    "data = pd.read_csv(file_path)\n",
    "required = ['Presion', 'Temperatura', 'Volumen', 'Cliente']\n",
    "missing  = [c for c in required if c not in data.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Faltan columnas esenciales: {missing}\")\n",
    "\n",
    "# convertir columnas numéricas y eliminar filas inválidas\n",
    "for c in ['Presion', 'Temperatura', 'Volumen']:\n",
    "    data[c] = pd.to_numeric(data[c], errors='coerce')\n",
    "before = len(data)\n",
    "data.dropna(subset=['Presion', 'Temperatura', 'Volumen'], inplace=True)\n",
    "after = len(data)\n",
    "print(f\"[1] Ingesta: {before} → {after} registros válidos\")\n",
    "\n",
    "# ------------------ 2. Clusterización de clientes ------------------\n",
    "# 2.1 Resumen medio por cliente\n",
    "summary = data.groupby('Cliente')[['Presion','Temperatura','Volumen']].mean()\n",
    "\n",
    "# 2.2 Escalado\n",
    "scaler   = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(summary)\n",
    "\n",
    "# 2.3 Imputación de NaNs, si los hay\n",
    "if np.isnan(X_scaled).any():\n",
    "    imputer  = SimpleImputer(strategy='mean')\n",
    "    X_scaled = imputer.fit_transform(X_scaled)\n",
    "\n",
    "# 2.4 KMeans\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=random_state, n_init=10)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "summary['cluster'] = labels\n",
    "\n",
    "# 2.5 Evaluación con Silhouette\n",
    "sil = silhouette_score(X_scaled, labels)\n",
    "print(f\"[2] Puntuación Silhouette: {sil:.3f}\")\n",
    "\n",
    "# 2.6 Guardando modelos\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "joblib.dump(kmeans, os.path.join(output_dir, 'kmeans_model.joblib'))\n",
    "joblib.dump(scaler, os.path.join(output_dir, 'scaler.joblib'))\n",
    "print(f\"[2] Modelos guardados en '{output_dir}'\")\n",
    "\n",
    "# ------------------ 3. Preparación de los datos para entrenamiento ------------------\n",
    "# 3.1 Unión de los clusters al dataset original\n",
    "df_clusters = summary.reset_index()[['Cliente','cluster']]\n",
    "data = data.merge(df_clusters, on='Cliente', how='left')\n",
    "\n",
    "# 3.2 Eliminar registros sin cluster\n",
    "b = len(data)\n",
    "data.dropna(subset=['cluster'], inplace=True)\n",
    "a = len(data)\n",
    "print(f\"[3] Eliminados {b-a} registros sin cluster\")\n",
    "data['cluster'] = data['cluster'].astype(int)\n",
    "\n",
    "# 3.3 Cálculo de Anomalia_Proxy_Cluster\n",
    "data['Anomalia_Proxy_Cluster'] = 0\n",
    "for cid in range(num_clusters):\n",
    "    mask = data['cluster'] == cid\n",
    "    sub  = data.loc[mask, ['Presion','Temperatura','Volumen']]\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    flags = pd.DataFrame(index=sub.index)\n",
    "    for feat in ['Presion','Temperatura','Volumen']:\n",
    "        p10, p90 = sub[feat].quantile([0.10, 0.90])\n",
    "        flags[feat] = sub[feat].apply(lambda x: 1 if x < p10 or x > p90 else 0)\n",
    "    data.loc[mask, 'Anomalia_Proxy_Cluster'] = flags.max(axis=1)\n",
    "print(\"[3] Columna 'Anomalia_Proxy_Cluster' calculada\")\n",
    "\n",
    "# 3.4 Renombrar columnas al formato final\n",
    "data.rename(columns={\n",
    "    'Cliente': 'cliente',\n",
    "    'Fecha': 'fecha',                 # en caso de que exista la columna Fecha\n",
    "    'Presion': 'presion',\n",
    "    'Temperatura': 'temperatura',\n",
    "    'Volumen': 'volumen',\n",
    "    'Mes': 'mes',\n",
    "    'Ano': 'año',                     # si la columna se llama Ano\n",
    "    'Presion_Suavizada': 'presion_suavizada',\n",
    "    'Es_Feriado': 'es_feriado',\n",
    "    'Anomalia': 'anomalia',\n",
    "    'Anomalia_bin': 'anomalia_bin'\n",
    "}, inplace=True)\n",
    "# Las columnas ya creadas abajo ya tienen el nombre correcto:\n",
    "# 'cluster', 'anomalia_presion', 'anomalia_temperatura',\n",
    "# 'anomalia_volumen', 'anomalia_proxy_cluster'\n",
    "\n",
    "# ------------------ 4. Guardado final ------------------\n",
    "training_path = \"training_data.csv\"\n",
    "data.to_csv(training_path, index=False)\n",
    "print(f\"[4] Datos de entrenamiento guardados en '{training_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce6b42f-3780-455d-890d-36069e66978d",
   "metadata": {},
   "source": [
    "# 2) Validación de datos Generados con Pandas por medio de Spark SQL (Pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2681ad93-2e49-4269-a762-38200e955a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. Criar SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ValidacaoTrainingData\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Ler o CSV e renomear colunas\n",
    "df_train = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"training_data.csv\") \\\n",
    "    .withColumnRenamed(\"año\", \"ano\") \\\n",
    "    .withColumnRenamed(\"Anomalia_Proxy_Cluster\", \"anomalia_proxy_cluster\")\n",
    "\n",
    "# 3. Registrar como view\n",
    "df_train.createOrReplaceTempView(\"training_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99f17d63-952e-4a7f-8d18-912dfdce6565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- presion: double (nullable = true)\n",
      " |-- temperatura: double (nullable = true)\n",
      " |-- volumen: double (nullable = true)\n",
      " |-- cliente: string (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      " |-- ano: integer (nullable = true)\n",
      " |-- es_feriado: boolean (nullable = true)\n",
      " |-- anomalia: string (nullable = true)\n",
      " |-- anomalia_bin: integer (nullable = true)\n",
      " |-- cluster: integer (nullable = true)\n",
      " |-- anomalia_proxy_cluster: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf88e41-9439-41b3-8e73-e1c3b32ab694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+--------+---+----+----------+--------+------------+-------+----------------------+\n",
      "|presion          |temperatura      |volumen          |cliente |mes|ano |es_feriado|anomalia|anomalia_bin|cluster|anomalia_proxy_cluster|\n",
      "+-----------------+-----------------+-----------------+--------+---+----+----------+--------+------------+-------+----------------------+\n",
      "|17.7325634924889 |28.2093536541928 |20.96975076975659|CLIENTE1|1  |2019|false     |Normal  |0           |4      |0                     |\n",
      "|17.74777603806793|28.51861421312152|17.84573913758869|CLIENTE1|1  |2019|false     |Normal  |0           |4      |0                     |\n",
      "|17.75891638774564|28.23019056507057|20.97591383522386|CLIENTE1|1  |2019|false     |Normal  |0           |4      |0                     |\n",
      "|17.72794022684193|27.8115085858999 |20.59229909002912|CLIENTE1|1  |2019|false     |Normal  |0           |4      |0                     |\n",
      "|17.74648374408114|27.79529343321371|21.69062581523098|CLIENTE1|1  |2019|false     |Normal  |0           |4      |0                     |\n",
      "|17.73554042397516|27.66645651943054|19.09171697331939|CLIENTE1|1  |2019|false     |Normal  |0           |4      |0                     |\n",
      "|17.73179953287394|27.53234547004581|18.94188520855138|CLIENTE1|1  |2019|false     |Normal  |0           |4      |0                     |\n",
      "|17.71530664648446|27.27956374846896|14.99414619209886|CLIENTE1|1  |2019|false     |Normal  |0           |4      |0                     |\n",
      "|17.70288300304363|27.08497753285612|19.29994101318612|CLIENTE1|1  |2019|false     |Normal  |0           |4      |0                     |\n",
      "|17.71153158996626|28.82525706474128|19.40866744770924|CLIENTE1|1  |2019|false     |Normal  |0           |4      |0                     |\n",
      "+-----------------+-----------------+-----------------+--------+---+----+----------+--------+------------+-------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta A: Mostrar las primeras 10 filas de muestra\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM training_data\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c6155c-ddf2-4053-92e0-b4f67607a03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|total_filas|\n",
      "+-----------+\n",
      "|     847960|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta B: Conteo total de filas\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      COUNT(*) AS total_filas\n",
    "    FROM training_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa48f189-e990-4429-a3a8-60df02a3cec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+------------+------------+--------+--------+---------------+-------------+-----------------+------------+---------------------------+\n",
      "|null_presion|null_temperatura|null_volumen|null_cliente|null_mes|null_ano|null_es_feriado|null_anomalia|null_anomalia_bin|null_cluster|null_anomalia_proxy_cluster|\n",
      "+------------+----------------+------------+------------+--------+--------+---------------+-------------+-----------------+------------+---------------------------+\n",
      "|           0|               0|           0|           0|       0|       0|              0|            0|                0|           0|                          0|\n",
      "+------------+----------------+------------+------------+--------+--------+---------------+-------------+-----------------+------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta C: Verificación de valores nulos por columna\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      SUM(CASE WHEN presion                IS NULL THEN 1 ELSE 0 END) AS null_presion,\n",
    "      SUM(CASE WHEN temperatura            IS NULL THEN 1 ELSE 0 END) AS null_temperatura,\n",
    "      SUM(CASE WHEN volumen                IS NULL THEN 1 ELSE 0 END) AS null_volumen,\n",
    "      SUM(CASE WHEN cliente                IS NULL THEN 1 ELSE 0 END) AS null_cliente,\n",
    "      SUM(CASE WHEN mes                    IS NULL THEN 1 ELSE 0 END) AS null_mes,\n",
    "      SUM(CASE WHEN ano                    IS NULL THEN 1 ELSE 0 END) AS null_ano,\n",
    "      SUM(CASE WHEN es_feriado             IS NULL THEN 1 ELSE 0 END) AS null_es_feriado,\n",
    "      SUM(CASE WHEN anomalia               IS NULL THEN 1 ELSE 0 END) AS null_anomalia,\n",
    "      SUM(CASE WHEN anomalia_bin           IS NULL THEN 1 ELSE 0 END) AS null_anomalia_bin,\n",
    "      SUM(CASE WHEN cluster                IS NULL THEN 1 ELSE 0 END) AS null_cluster,\n",
    "      SUM(CASE WHEN anomalia_proxy_cluster IS NULL THEN 1 ELSE 0 END) AS null_anomalia_proxy_cluster\n",
    "    FROM training_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d301757-c052-4394-a7d0-00a8fb848de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|cluster| total|\n",
      "+-------+------+\n",
      "|      0|171910|\n",
      "|      1|336775|\n",
      "|      2| 84496|\n",
      "|      3| 82764|\n",
      "|      4|172015|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta D: Distribución de clusters\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      cluster,\n",
    "      COUNT(*) AS total\n",
    "    FROM training_data\n",
    "    GROUP BY cluster\n",
    "    ORDER BY cluster\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7371a4e-13a3-4857-90e3-acaaa42f33a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------+\n",
      "|anomalia_proxy_cluster| total|\n",
      "+----------------------+------+\n",
      "|                     0|496454|\n",
      "|                     1|351506|\n",
      "+----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta E: Distribución de anomalia_proxy_cluster\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      anomalia_proxy_cluster,\n",
    "      COUNT(*) AS total\n",
    "    FROM training_data\n",
    "    GROUP BY anomalia_proxy_cluster\n",
    "    ORDER BY anomalia_proxy_cluster\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44d5121b-3d5c-4b8a-a091-6dc875282a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------+------+\n",
      "|anomalia_bin|anomalia_proxy_cluster| total|\n",
      "+------------+----------------------+------+\n",
      "|           0|                     0|462350|\n",
      "|           0|                     1|253860|\n",
      "|           1|                     0| 34104|\n",
      "|           1|                     1| 97646|\n",
      "+------------+----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta F: Consistencia entre anomalia_bin y anomalia_proxy_cluster\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      anomalia_bin,\n",
    "      anomalia_proxy_cluster,\n",
    "      COUNT(*) AS total\n",
    "    FROM training_data\n",
    "    GROUP BY anomalia_bin, anomalia_proxy_cluster\n",
    "    ORDER BY anomalia_bin, anomalia_proxy_cluster\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "520f70cd-6481-4df9-8b1e-8e9573f42021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+---------------+---------------+---------------+-----------+-----------+-----------+\n",
      "|avg_presion|min_presion|max_presion|avg_temperatura|min_temperatura|max_temperatura|avg_volumen|min_volumen|max_volumen|\n",
      "+-----------+-----------+-----------+---------------+---------------+---------------+-----------+-----------+-----------+\n",
      "|16.07      |2.93       |20.31      |25.2           |-5.26          |50.02          |62.33      |0.0        |577.41     |\n",
      "+-----------+-----------+-----------+---------------+---------------+---------------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta G: Estadísticas básicas de las columnas numéricas\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      ROUND(AVG(presion),2)     AS avg_presion,\n",
    "      ROUND(MIN(presion),2)     AS min_presion,\n",
    "      ROUND(MAX(presion),2)     AS max_presion,\n",
    "      ROUND(AVG(temperatura),2) AS avg_temperatura,\n",
    "      ROUND(MIN(temperatura),2) AS min_temperatura,\n",
    "      ROUND(MAX(temperatura),2) AS max_temperatura,\n",
    "      ROUND(AVG(volumen),2)     AS avg_volumen,\n",
    "      ROUND(MIN(volumen),2)     AS min_volumen,\n",
    "      ROUND(MAX(volumen),2)     AS max_volumen\n",
    "    FROM training_data\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0842e40-6f1d-43b3-af4b-e63ba8e173b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|cluster|pct_proxy_anomalia|\n",
      "+-------+------------------+\n",
      "|      0|             43.59|\n",
      "|      1|             38.76|\n",
      "|      2|             46.12|\n",
      "|      3|             50.00|\n",
      "|      4|             38.19|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta H: Porcentaje de proxy-anomalías por cluster\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      cluster,\n",
    "      ROUND(100.0 * SUM(anomalia_proxy_cluster) / COUNT(*),2) AS pct_proxy_anomalia\n",
    "    FROM training_data\n",
    "    GROUP BY cluster\n",
    "    ORDER BY cluster\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f9e6808-0239-4588-8226-2f1671fc0eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---+--------------+\n",
      "|cliente |ano |mes|qtde_registros|\n",
      "+--------+----+---+--------------+\n",
      "|CLIENTE1|2019|1  |431           |\n",
      "|CLIENTE1|2019|2  |672           |\n",
      "|CLIENTE1|2019|3  |743           |\n",
      "|CLIENTE1|2019|4  |720           |\n",
      "|CLIENTE1|2019|5  |744           |\n",
      "|CLIENTE1|2019|6  |720           |\n",
      "|CLIENTE1|2019|7  |744           |\n",
      "|CLIENTE1|2019|8  |744           |\n",
      "|CLIENTE1|2019|9  |720           |\n",
      "|CLIENTE1|2019|10 |744           |\n",
      "+--------+----+---+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta I: Cobertura temporal por cliente\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      cliente,\n",
    "      ano,\n",
    "      mes,\n",
    "      COUNT(*) AS qtde_registros\n",
    "    FROM training_data\n",
    "    GROUP BY cliente, ano, mes\n",
    "    ORDER BY cliente, ano, mes\n",
    "\"\"\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ffcd28-c834-48af-b085-54e8b0daf0a1",
   "metadata": {},
   "source": [
    "# 3) Validación de datos Generados con AWS Glue/Pyspark por medio de Spark SQL (Pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5da20ea",
   "metadata": {},
   "source": [
    "Estos archivos que se leerán a continuación fueron extraídos del bucket de la capa refined del data lake para validar los datos, es decir, para comprobar si lo que se generó con Glue y PySpark coincide con lo que se había hecho inicialmente con pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "687fddd4-1ef9-408e-96cf-27df06d3a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Leitura e rename em df_refined\n",
    "df_refined = (\n",
    "    spark.read\n",
    "         .parquet(\"Parquet_Refined_Files/part-*.snappy.parquet\")\n",
    "         .withColumnRenamed(\"año\", \"ano\")\n",
    "         .withColumnRenamed(\"Anomalia_Proxy_Cluster\", \"anomalia_proxy_cluster\")\n",
    ")\n",
    "\n",
    "# 2) Registrar a view usando df_refined, não 'df'\n",
    "df_refined.createOrReplaceTempView(\"contugas_refined_dato_procesado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df7f1848-7400-4a19-92e2-a5c8729a7744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------------------+-----------------+-----------------+-----------------+---+----+------------------+----------+-----------------+------------------+--------+------------+----------------------+\n",
      "|cluster|cliente  |fecha              |presion          |temperatura      |volumen          |mes|ano |presion_suavizada |es_feriado|p10              |p90               |anomalia|anomalia_bin|anomalia_proxy_cluster|\n",
      "+-------+---------+-------------------+-----------------+-----------------+-----------------+---+----+------------------+----------+-----------------+------------------+--------+------------+----------------------+\n",
      "|2      |CLIENTE11|2021-01-01 16:00:00|3.555825750286133|27.47967341667485|110.8325498819195|1  |2021|15.608226364489315|true      |84.15590463899385|171.35187757400683|Normal  |0           |0                     |\n",
      "|2      |CLIENTE11|2021-04-03 03:00:00|3.645878675455474|29.92690024787693|105.7093616999356|4  |2021|16.059413956351662|false     |84.15590463899385|171.35187757400683|Normal  |0           |1                     |\n",
      "|2      |CLIENTE11|2020-08-14 07:00:00|3.371700248637208|22.9756909405708 |204.1965174376297|8  |2020|15.698019911438037|false     |84.15590463899385|171.35187757400683|Anómalo |1           |1                     |\n",
      "|2      |CLIENTE11|2020-11-10 01:00:00|3.576821290744107|25.30488921413348|102.477199142165 |11 |2020|15.788603062763638|false     |84.15590463899385|171.35187757400683|Normal  |0           |0                     |\n",
      "|2      |CLIENTE11|2019-05-27 14:00:00|3.600022123749569|25.72422757513995|108.0339905414466|5  |2019|15.593581694891515|false     |84.15590463899385|171.35187757400683|Normal  |0           |0                     |\n",
      "|2      |CLIENTE11|2021-02-13 01:00:00|3.700241961240447|28.57198518666797|85.47211350384349|2  |2021|15.799310560329403|false     |84.15590463899385|171.35187757400683|Normal  |0           |1                     |\n",
      "|2      |CLIENTE11|2023-03-17 07:00:00|3.448340286445211|32.11682691802301|145.5809519772081|3  |2023|15.565615549755575|false     |84.15590463899385|171.35187757400683|Normal  |0           |1                     |\n",
      "|2      |CLIENTE11|2021-02-17 04:00:00|3.838399482017882|29.34868667489422|72.14702846804539|2  |2021|15.826426287815767|false     |84.15590463899385|171.35187757400683|Anómalo |1           |1                     |\n",
      "|2      |CLIENTE11|2023-05-29 23:00:00|3.559715759627115|29.00944272648294|132.6522532059512|5  |2023|15.541790322317603|false     |84.15590463899385|171.35187757400683|Normal  |0           |0                     |\n",
      "|2      |CLIENTE11|2020-09-23 06:00:00|3.495852611704942|23.22895425158896|172.0255824087841|9  |2020|15.75220776234596 |false     |84.15590463899385|171.35187757400683|Anómalo |1           |1                     |\n",
      "+-------+---------+-------------------+-----------------+-----------------+-----------------+---+----+------------------+----------+-----------------+------------------+--------+------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta A: Mostrar las primeras 10 filas de muestra\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM contugas_refined_dato_procesado\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2885c31-5b0c-48d5-90f7-2a93acad6973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|total_filas|\n",
      "+-----------+\n",
      "|     847960|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta B: Conteo total de filas\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      COUNT(*) AS total_filas\n",
    "    FROM contugas_refined_dato_procesado\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0623bda0-b9fa-4704-9aba-039a9c574d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+------------+------------+--------+--------+---------------+-------------+-----------------+------------+---------------------------+\n",
      "|null_presion|null_temperatura|null_volumen|null_cliente|null_mes|null_ano|null_es_feriado|null_anomalia|null_anomalia_bin|null_cluster|null_anomalia_proxy_cluster|\n",
      "+------------+----------------+------------+------------+--------+--------+---------------+-------------+-----------------+------------+---------------------------+\n",
      "|           0|               0|           0|           0|       0|       0|              0|            0|                0|           0|                          0|\n",
      "+------------+----------------+------------+------------+--------+--------+---------------+-------------+-----------------+------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta C: Verificación de valores nulos por columna\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      SUM(CASE WHEN presion                IS NULL THEN 1 ELSE 0 END) AS null_presion,\n",
    "      SUM(CASE WHEN temperatura            IS NULL THEN 1 ELSE 0 END) AS null_temperatura,\n",
    "      SUM(CASE WHEN volumen                IS NULL THEN 1 ELSE 0 END) AS null_volumen,\n",
    "      SUM(CASE WHEN cliente                IS NULL THEN 1 ELSE 0 END) AS null_cliente,\n",
    "      SUM(CASE WHEN mes                    IS NULL THEN 1 ELSE 0 END) AS null_mes,\n",
    "      SUM(CASE WHEN ano                    IS NULL THEN 1 ELSE 0 END) AS null_ano,\n",
    "      SUM(CASE WHEN es_feriado             IS NULL THEN 1 ELSE 0 END) AS null_es_feriado,\n",
    "      SUM(CASE WHEN anomalia               IS NULL THEN 1 ELSE 0 END) AS null_anomalia,\n",
    "      SUM(CASE WHEN anomalia_bin           IS NULL THEN 1 ELSE 0 END) AS null_anomalia_bin,\n",
    "      SUM(CASE WHEN cluster                IS NULL THEN 1 ELSE 0 END) AS null_cluster,\n",
    "      SUM(CASE WHEN anomalia_proxy_cluster IS NULL THEN 1 ELSE 0 END) AS null_anomalia_proxy_cluster\n",
    "    FROM contugas_refined_dato_procesado\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "095807ff-e1ea-4385-8947-736fd86f1b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|cluster| total|\n",
      "+-------+------+\n",
      "|      0|336775|\n",
      "|      1|171910|\n",
      "|      2| 84496|\n",
      "|      3| 82764|\n",
      "|      4|172015|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta D: Distribución de clusters\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      cluster,\n",
    "      COUNT(*) AS total\n",
    "    FROM contugas_refined_dato_procesado\n",
    "    GROUP BY cluster\n",
    "    ORDER BY cluster\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "359f3759-25d4-401a-920a-0eaad1261e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------+\n",
      "|anomalia_proxy_cluster| total|\n",
      "+----------------------+------+\n",
      "|                     0|496413|\n",
      "|                     1|351547|\n",
      "+----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta E: Distribución de anomalia_proxy_cluster\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      anomalia_proxy_cluster,\n",
    "      COUNT(*) AS total\n",
    "    FROM contugas_refined_dato_procesado\n",
    "    GROUP BY anomalia_proxy_cluster\n",
    "    ORDER BY anomalia_proxy_cluster\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16470882-a571-4060-9ce2-3b3aa730d6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------+------+\n",
      "|anomalia_bin|anomalia_proxy_cluster| total|\n",
      "+------------+----------------------+------+\n",
      "|           0|                     0|462330|\n",
      "|           0|                     1|253880|\n",
      "|           1|                     0| 34083|\n",
      "|           1|                     1| 97667|\n",
      "+------------+----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta F: Consistencia entre anomalia_bin y anomalia_proxy_cluster\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      anomalia_bin,\n",
    "      anomalia_proxy_cluster,\n",
    "      COUNT(*) AS total\n",
    "    FROM contugas_refined_dato_procesado\n",
    "    GROUP BY anomalia_bin, anomalia_proxy_cluster\n",
    "    ORDER BY anomalia_bin, anomalia_proxy_cluster\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92800ca5-028e-464b-b249-d89861078d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+---------------+---------------+---------------+-----------+-----------+-----------+\n",
      "|avg_presion|min_presion|max_presion|avg_temperatura|min_temperatura|max_temperatura|avg_volumen|min_volumen|max_volumen|\n",
      "+-----------+-----------+-----------+---------------+---------------+---------------+-----------+-----------+-----------+\n",
      "|16.07      |2.93       |20.31      |25.2           |-5.26          |50.02          |62.33      |0.0        |577.41     |\n",
      "+-----------+-----------+-----------+---------------+---------------+---------------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta G: Estadísticas básicas de las columnas numéricas\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      ROUND(AVG(presion),2)       AS avg_presion,\n",
    "      ROUND(MIN(presion),2)       AS min_presion,\n",
    "      ROUND(MAX(presion),2)       AS max_presion,\n",
    "      ROUND(AVG(temperatura),2)   AS avg_temperatura,\n",
    "      ROUND(MIN(temperatura),2)   AS min_temperatura,\n",
    "      ROUND(MAX(temperatura),2)   AS max_temperatura,\n",
    "      ROUND(AVG(volumen),2)       AS avg_volumen,\n",
    "      ROUND(MIN(volumen),2)       AS min_volumen,\n",
    "      ROUND(MAX(volumen),2)       AS max_volumen\n",
    "    FROM contugas_refined_dato_procesado\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "566b230b-4b7b-43ba-a4ea-08f28080c482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|cluster|pct_proxy_anomalia|\n",
      "+-------+------------------+\n",
      "|      0|             38.76|\n",
      "|      1|             43.60|\n",
      "|      2|             46.13|\n",
      "|      3|             50.00|\n",
      "|      4|             38.19|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta H: Porcentaje de anomalias proxy por cluster\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      cluster,\n",
    "      ROUND(100.0 * SUM(anomalia_proxy_cluster) / COUNT(*),2) AS pct_proxy_anomalia\n",
    "    FROM contugas_refined_dato_procesado\n",
    "    GROUP BY cluster\n",
    "    ORDER BY cluster\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46792fc6-ee19-40f4-a14a-031ff2e6ad69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---+--------------+\n",
      "|cliente |ano |mes|qtde_registros|\n",
      "+--------+----+---+--------------+\n",
      "|CLIENTE1|2019|1  |431           |\n",
      "|CLIENTE1|2019|2  |672           |\n",
      "|CLIENTE1|2019|3  |743           |\n",
      "|CLIENTE1|2019|4  |720           |\n",
      "|CLIENTE1|2019|5  |744           |\n",
      "|CLIENTE1|2019|6  |720           |\n",
      "|CLIENTE1|2019|7  |744           |\n",
      "|CLIENTE1|2019|8  |744           |\n",
      "|CLIENTE1|2019|9  |720           |\n",
      "|CLIENTE1|2019|10 |744           |\n",
      "+--------+----+---+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta I: Cobertura temporal por cliente\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      cliente,\n",
    "      ano,\n",
    "      mes,\n",
    "      COUNT(*) AS qtde_registros\n",
    "    FROM contugas_refined_dato_procesado\n",
    "    GROUP BY cliente, ano, mes\n",
    "    ORDER BY cliente, ano, mes\n",
    "\"\"\").show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
